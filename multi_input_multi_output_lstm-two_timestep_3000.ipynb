{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>V</th>\n",
       "      <th>AP</th>\n",
       "      <th>RH</th>\n",
       "      <th>PE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.64</td>\n",
       "      <td>58.49</td>\n",
       "      <td>1011.40</td>\n",
       "      <td>74.20</td>\n",
       "      <td>445.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.74</td>\n",
       "      <td>56.90</td>\n",
       "      <td>1007.15</td>\n",
       "      <td>41.91</td>\n",
       "      <td>438.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.07</td>\n",
       "      <td>49.69</td>\n",
       "      <td>1007.22</td>\n",
       "      <td>76.79</td>\n",
       "      <td>453.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.80</td>\n",
       "      <td>40.66</td>\n",
       "      <td>1017.13</td>\n",
       "      <td>97.20</td>\n",
       "      <td>464.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.97</td>\n",
       "      <td>39.16</td>\n",
       "      <td>1016.05</td>\n",
       "      <td>84.60</td>\n",
       "      <td>470.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9562</th>\n",
       "      <td>15.12</td>\n",
       "      <td>48.92</td>\n",
       "      <td>1011.80</td>\n",
       "      <td>72.93</td>\n",
       "      <td>462.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9563</th>\n",
       "      <td>33.41</td>\n",
       "      <td>77.95</td>\n",
       "      <td>1010.30</td>\n",
       "      <td>59.72</td>\n",
       "      <td>432.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9564</th>\n",
       "      <td>15.99</td>\n",
       "      <td>43.34</td>\n",
       "      <td>1014.20</td>\n",
       "      <td>78.66</td>\n",
       "      <td>465.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9565</th>\n",
       "      <td>17.65</td>\n",
       "      <td>59.87</td>\n",
       "      <td>1018.58</td>\n",
       "      <td>94.65</td>\n",
       "      <td>450.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9566</th>\n",
       "      <td>23.68</td>\n",
       "      <td>51.30</td>\n",
       "      <td>1011.86</td>\n",
       "      <td>71.24</td>\n",
       "      <td>451.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9567 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AT      V       AP     RH      PE\n",
       "0     23.64  58.49  1011.40  74.20  445.75\n",
       "1     29.74  56.90  1007.15  41.91  438.76\n",
       "2     19.07  49.69  1007.22  76.79  453.09\n",
       "3     11.80  40.66  1017.13  97.20  464.43\n",
       "4     13.97  39.16  1016.05  84.60  470.96\n",
       "...     ...    ...      ...    ...     ...\n",
       "9562  15.12  48.92  1011.80  72.93  462.59\n",
       "9563  33.41  77.95  1010.30  59.72  432.90\n",
       "9564  15.99  43.34  1014.20  78.66  465.96\n",
       "9565  17.65  59.87  1018.58  94.65  450.93\n",
       "9566  23.68  51.30  1011.86  71.24  451.67\n",
       "\n",
       "[9567 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # pandas library use for data analysis\n",
    "#first manually convert xls to csv\n",
    "data = pd.read_csv(\"Folds_test.csv\") # import csv file as dataframe\n",
    "data # show dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=  data.head(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>V</th>\n",
       "      <th>AP</th>\n",
       "      <th>RH</th>\n",
       "      <th>PE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.64</td>\n",
       "      <td>58.49</td>\n",
       "      <td>1011.40</td>\n",
       "      <td>74.20</td>\n",
       "      <td>445.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.74</td>\n",
       "      <td>56.90</td>\n",
       "      <td>1007.15</td>\n",
       "      <td>41.91</td>\n",
       "      <td>438.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.07</td>\n",
       "      <td>49.69</td>\n",
       "      <td>1007.22</td>\n",
       "      <td>76.79</td>\n",
       "      <td>453.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.80</td>\n",
       "      <td>40.66</td>\n",
       "      <td>1017.13</td>\n",
       "      <td>97.20</td>\n",
       "      <td>464.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.97</td>\n",
       "      <td>39.16</td>\n",
       "      <td>1016.05</td>\n",
       "      <td>84.60</td>\n",
       "      <td>470.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>20.04</td>\n",
       "      <td>48.92</td>\n",
       "      <td>1011.14</td>\n",
       "      <td>68.92</td>\n",
       "      <td>447.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>9.37</td>\n",
       "      <td>42.32</td>\n",
       "      <td>1015.15</td>\n",
       "      <td>82.96</td>\n",
       "      <td>477.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>20.76</td>\n",
       "      <td>58.18</td>\n",
       "      <td>1007.80</td>\n",
       "      <td>99.22</td>\n",
       "      <td>449.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>31.17</td>\n",
       "      <td>69.51</td>\n",
       "      <td>1010.51</td>\n",
       "      <td>43.11</td>\n",
       "      <td>428.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>21.74</td>\n",
       "      <td>70.32</td>\n",
       "      <td>1011.88</td>\n",
       "      <td>85.95</td>\n",
       "      <td>439.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AT      V       AP     RH      PE\n",
       "0     23.64  58.49  1011.40  74.20  445.75\n",
       "1     29.74  56.90  1007.15  41.91  438.76\n",
       "2     19.07  49.69  1007.22  76.79  453.09\n",
       "3     11.80  40.66  1017.13  97.20  464.43\n",
       "4     13.97  39.16  1016.05  84.60  470.96\n",
       "...     ...    ...      ...    ...     ...\n",
       "2995  20.04  48.92  1011.14  68.92  447.89\n",
       "2996   9.37  42.32  1015.15  82.96  477.57\n",
       "2997  20.76  58.18  1007.80  99.22  449.25\n",
       "2998  31.17  69.51  1010.51  43.11  428.94\n",
       "2999  21.74  70.32  1011.88  85.95  439.19\n",
       "\n",
       "[3000 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # numpy is library use for matrix manipulation\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps): \n",
    "    X, y = list(), list() # initilize list for feature matrixs and labels\n",
    "    for i in range(len(sequence)): # loop through number of sequence \n",
    "        end_ix = i + n_steps # initilize end value  \n",
    "        if end_ix > len(sequence)-1: # check if end value is greater than length of sequence \n",
    "            break # use break to termninate the loop\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] # seperate features and labels\n",
    "        X.append(seq_x) # save feature in X list\n",
    "        y.append(seq_y) # save label in Y lisr\n",
    "    return np.array(X), np.array(y) # return X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1= np.array(data['AT'].values)  # save values of AT column in x1\n",
    "x2= np.array(data['V'].values)  # save values of V column in x2\n",
    "x3= np.array(data['AP'].values)  # save values of AP column in x3\n",
    "x4= np.array(data['RH'].values)  # save values of RH column in x4\n",
    "x5= np.array(data['PE'].values) # save values of PE column in x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=4\n",
    "x_1, y_1 = split_sequence(x1, seq_len) # call split sequence function \n",
    "x_2, y_2 = split_sequence(x2, seq_len) # call split sequence function \n",
    "x_3, y_3 = split_sequence(x3, seq_len) # call split sequence function \n",
    "x_4, y_4 = split_sequence(x4, seq_len) # call split sequence function \n",
    "x_5, y_5 = split_sequence(x5, seq_len) # call split sequence function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import hstack # use hstack to merger different columns\n",
    "X = hstack((x_1, x_2,x_3,x_4,x_5)) # merger 5 featuers into one matrix X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1 = y_1.reshape((-1, 1)) # convert vector row to vector column matrix\n",
    "y_2 = y_2.reshape((-1, 1)) # convert vector row to vector column matrix\n",
    "y_3 = y_3.reshape((-1, 1)) # convert vector row to vector column matrix\n",
    "y_4 = y_4.reshape((-1, 1)) # convert vector row to vector column matrix\n",
    "y_5 = y_5.reshape((-1, 1)) # convert vector row to vector column matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.column_stack((y_1,y_2,y_3,y_4,y_5)) # use column wise merge to 5 labels matrix into Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2996, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape) # \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "Y = scaler.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (2996, 20, 1) y: (2996, 5)\n",
      "(20, 1)\n",
      "5\n",
      "xtrain: (2396, 20, 1) ytrian: (2396, 5)\n",
      "WARNING:tensorflow:From C:\\Users\\Mohammad Faizan Khan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 20, 32)            4352      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 29,509\n",
      "Trainable params: 29,509\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2156 samples, validate on 240 samples\n",
      "WARNING:tensorflow:From C:\\Users\\Mohammad Faizan Khan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/150\n",
      "2156/2156 - 6s - loss: 0.0809 - val_loss: 0.0440\n",
      "Epoch 2/150\n",
      "2156/2156 - 3s - loss: 0.0453 - val_loss: 0.0439\n",
      "Epoch 3/150\n",
      "2156/2156 - 3s - loss: 0.0454 - val_loss: 0.0444\n",
      "Epoch 4/150\n",
      "2156/2156 - 3s - loss: 0.0454 - val_loss: 0.0438\n",
      "Epoch 5/150\n",
      "2156/2156 - 3s - loss: 0.0453 - val_loss: 0.0442\n",
      "Epoch 6/150\n",
      "2156/2156 - 3s - loss: 0.0451 - val_loss: 0.0442\n",
      "Epoch 7/150\n",
      "2156/2156 - 3s - loss: 0.0448 - val_loss: 0.0440\n",
      "Epoch 8/150\n",
      "2156/2156 - 3s - loss: 0.0451 - val_loss: 0.0438\n",
      "Epoch 9/150\n",
      "2156/2156 - 3s - loss: 0.0449 - val_loss: 0.0440\n",
      "Epoch 10/150\n",
      "2156/2156 - 3s - loss: 0.0448 - val_loss: 0.0441\n",
      "Epoch 11/150\n",
      "2156/2156 - 3s - loss: 0.0449 - val_loss: 0.0439\n",
      "Epoch 12/150\n",
      "2156/2156 - 3s - loss: 0.0450 - val_loss: 0.0442\n",
      "Epoch 13/150\n",
      "2156/2156 - 3s - loss: 0.0449 - val_loss: 0.0438\n",
      "Epoch 14/150\n",
      "2156/2156 - 3s - loss: 0.0449 - val_loss: 0.0438\n",
      "Epoch 15/150\n",
      "2156/2156 - 3s - loss: 0.0449 - val_loss: 0.0439\n",
      "Epoch 16/150\n",
      "2156/2156 - 3s - loss: 0.0449 - val_loss: 0.0437\n",
      "Epoch 17/150\n",
      "2156/2156 - 3s - loss: 0.0450 - val_loss: 0.0437\n",
      "Epoch 18/150\n",
      "2156/2156 - 3s - loss: 0.0451 - val_loss: 0.0437\n",
      "Epoch 19/150\n",
      "2156/2156 - 3s - loss: 0.0447 - val_loss: 0.0444\n",
      "Epoch 20/150\n",
      "2156/2156 - 3s - loss: 0.0447 - val_loss: 0.0442\n",
      "Epoch 21/150\n",
      "2156/2156 - 3s - loss: 0.0448 - val_loss: 0.0439\n",
      "Epoch 22/150\n",
      "2156/2156 - 2s - loss: 0.0447 - val_loss: 0.0438\n",
      "Epoch 23/150\n",
      "2156/2156 - 2s - loss: 0.0447 - val_loss: 0.0438\n",
      "Epoch 24/150\n",
      "2156/2156 - 3s - loss: 0.0449 - val_loss: 0.0440\n",
      "Epoch 25/150\n",
      "2156/2156 - 3s - loss: 0.0448 - val_loss: 0.0438\n",
      "Epoch 26/150\n",
      "2156/2156 - 3s - loss: 0.0448 - val_loss: 0.0439\n",
      "Epoch 27/150\n",
      "2156/2156 - 3s - loss: 0.0447 - val_loss: 0.0439\n",
      "Epoch 28/150\n",
      "2156/2156 - 3s - loss: 0.0448 - val_loss: 0.0439\n",
      "Epoch 29/150\n",
      "2156/2156 - 3s - loss: 0.0448 - val_loss: 0.0437\n",
      "Epoch 30/150\n",
      "2156/2156 - 3s - loss: 0.0447 - val_loss: 0.0437\n",
      "Epoch 31/150\n",
      "2156/2156 - 3s - loss: 0.0447 - val_loss: 0.0440\n",
      "Epoch 32/150\n",
      "2156/2156 - 3s - loss: 0.0447 - val_loss: 0.0440\n",
      "Epoch 33/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0440\n",
      "Epoch 34/150\n",
      "2156/2156 - 3s - loss: 0.0447 - val_loss: 0.0438\n",
      "Epoch 35/150\n",
      "2156/2156 - 3s - loss: 0.0447 - val_loss: 0.0439\n",
      "Epoch 36/150\n",
      "2156/2156 - 3s - loss: 0.0448 - val_loss: 0.0443\n",
      "Epoch 37/150\n",
      "2156/2156 - 3s - loss: 0.0448 - val_loss: 0.0437\n",
      "Epoch 38/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0437\n",
      "Epoch 39/150\n",
      "2156/2156 - 3s - loss: 0.0450 - val_loss: 0.0438\n",
      "Epoch 40/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0438\n",
      "Epoch 41/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0441\n",
      "Epoch 42/150\n",
      "2156/2156 - 3s - loss: 0.0448 - val_loss: 0.0439\n",
      "Epoch 43/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0438\n",
      "Epoch 44/150\n",
      "2156/2156 - 2s - loss: 0.0446 - val_loss: 0.0437\n",
      "Epoch 45/150\n",
      "2156/2156 - 2s - loss: 0.0447 - val_loss: 0.0439\n",
      "Epoch 46/150\n",
      "2156/2156 - 2s - loss: 0.0446 - val_loss: 0.0438\n",
      "Epoch 47/150\n",
      "2156/2156 - 2s - loss: 0.0447 - val_loss: 0.0437\n",
      "Epoch 48/150\n",
      "2156/2156 - 2s - loss: 0.0446 - val_loss: 0.0439\n",
      "Epoch 49/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0440\n",
      "Epoch 50/150\n",
      "2156/2156 - 2s - loss: 0.0446 - val_loss: 0.0438\n",
      "Epoch 51/150\n",
      "2156/2156 - 2s - loss: 0.0446 - val_loss: 0.0437\n",
      "Epoch 52/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0436\n",
      "Epoch 53/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0438\n",
      "Epoch 54/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0438\n",
      "Epoch 55/150\n",
      "2156/2156 - 3s - loss: 0.0447 - val_loss: 0.0438\n",
      "Epoch 56/150\n",
      "2156/2156 - 2s - loss: 0.0447 - val_loss: 0.0439\n",
      "Epoch 57/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0437\n",
      "Epoch 58/150\n",
      "2156/2156 - 2s - loss: 0.0446 - val_loss: 0.0440\n",
      "Epoch 59/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0442\n",
      "Epoch 60/150\n",
      "2156/2156 - 2s - loss: 0.0446 - val_loss: 0.0436\n",
      "Epoch 61/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0439\n",
      "Epoch 62/150\n",
      "2156/2156 - 2s - loss: 0.0446 - val_loss: 0.0437\n",
      "Epoch 63/150\n",
      "2156/2156 - 2s - loss: 0.0447 - val_loss: 0.0436\n",
      "Epoch 64/150\n",
      "2156/2156 - 2s - loss: 0.0447 - val_loss: 0.0437\n",
      "Epoch 65/150\n",
      "2156/2156 - 2s - loss: 0.0445 - val_loss: 0.0437\n",
      "Epoch 66/150\n",
      "2156/2156 - 2s - loss: 0.0446 - val_loss: 0.0437\n",
      "Epoch 67/150\n",
      "2156/2156 - 2s - loss: 0.0446 - val_loss: 0.0436\n",
      "Epoch 68/150\n",
      "2156/2156 - 2s - loss: 0.0447 - val_loss: 0.0438\n",
      "Epoch 69/150\n",
      "2156/2156 - 3s - loss: 0.0447 - val_loss: 0.0441\n",
      "Epoch 70/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0441\n",
      "Epoch 71/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0436\n",
      "Epoch 72/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0437\n",
      "Epoch 73/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0439\n",
      "Epoch 74/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0440\n",
      "Epoch 75/150\n",
      "2156/2156 - 3s - loss: 0.0447 - val_loss: 0.0436\n",
      "Epoch 76/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0437\n",
      "Epoch 77/150\n",
      "2156/2156 - 2s - loss: 0.0445 - val_loss: 0.0437\n",
      "Epoch 78/150\n",
      "2156/2156 - 3s - loss: 0.0447 - val_loss: 0.0437\n",
      "Epoch 79/150\n",
      "2156/2156 - 3s - loss: 0.0447 - val_loss: 0.0436\n",
      "Epoch 80/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0437\n",
      "Epoch 81/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0439\n",
      "Epoch 82/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0436\n",
      "Epoch 83/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0435\n",
      "Epoch 84/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0436\n",
      "Epoch 85/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0437\n",
      "Epoch 86/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0435\n",
      "Epoch 87/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0437\n",
      "Epoch 88/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0438\n",
      "Epoch 89/150\n",
      "2156/2156 - 2s - loss: 0.0445 - val_loss: 0.0437\n",
      "Epoch 90/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0436\n",
      "Epoch 91/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0437\n",
      "Epoch 92/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0436\n",
      "Epoch 93/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0437\n",
      "Epoch 94/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0437\n",
      "Epoch 95/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0438\n",
      "Epoch 96/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0437\n",
      "Epoch 97/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0438\n",
      "Epoch 98/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0436\n",
      "Epoch 99/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0439\n",
      "Epoch 100/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0436\n",
      "Epoch 101/150\n",
      "2156/2156 - 2s - loss: 0.0445 - val_loss: 0.0439\n",
      "Epoch 102/150\n",
      "2156/2156 - 2s - loss: 0.0446 - val_loss: 0.0436\n",
      "Epoch 103/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0437\n",
      "Epoch 104/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0437\n",
      "Epoch 105/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0438\n",
      "Epoch 106/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0437\n",
      "Epoch 107/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0436\n",
      "Epoch 109/150\n",
      "2156/2156 - 3s - loss: 0.0444 - val_loss: 0.0436\n",
      "Epoch 110/150\n",
      "2156/2156 - 2s - loss: 0.0445 - val_loss: 0.0436\n",
      "Epoch 111/150\n",
      "2156/2156 - 2s - loss: 0.0446 - val_loss: 0.0435\n",
      "Epoch 112/150\n",
      "2156/2156 - 2s - loss: 0.0446 - val_loss: 0.0435\n",
      "Epoch 113/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0436\n",
      "Epoch 114/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0436\n",
      "Epoch 115/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0440\n",
      "Epoch 116/150\n",
      "2156/2156 - 3s - loss: 0.0447 - val_loss: 0.0437\n",
      "Epoch 117/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0436\n",
      "Epoch 118/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0436\n",
      "Epoch 119/150\n",
      "2156/2156 - 3s - loss: 0.0444 - val_loss: 0.0436\n",
      "Epoch 120/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0435\n",
      "Epoch 121/150\n",
      "2156/2156 - 2s - loss: 0.0445 - val_loss: 0.0436\n",
      "Epoch 122/150\n",
      "2156/2156 - 3s - loss: 0.0444 - val_loss: 0.0436\n",
      "Epoch 123/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0436\n",
      "Epoch 124/150\n",
      "2156/2156 - 3s - loss: 0.0444 - val_loss: 0.0436\n",
      "Epoch 125/150\n",
      "2156/2156 - 2s - loss: 0.0445 - val_loss: 0.0438\n",
      "Epoch 126/150\n",
      "2156/2156 - 3s - loss: 0.0446 - val_loss: 0.0436\n",
      "Epoch 127/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0436\n",
      "Epoch 128/150\n",
      "2156/2156 - 3s - loss: 0.0444 - val_loss: 0.0436\n",
      "Epoch 129/150\n",
      "2156/2156 - 3s - loss: 0.0444 - val_loss: 0.0436\n",
      "Epoch 130/150\n",
      "2156/2156 - 3s - loss: 0.0444 - val_loss: 0.0437\n",
      "Epoch 131/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0437\n",
      "Epoch 132/150\n",
      "2156/2156 - 3s - loss: 0.0444 - val_loss: 0.0435\n",
      "Epoch 133/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0437\n",
      "Epoch 134/150\n",
      "2156/2156 - 3s - loss: 0.0444 - val_loss: 0.0436\n",
      "Epoch 135/150\n",
      "2156/2156 - 3s - loss: 0.0444 - val_loss: 0.0435\n",
      "Epoch 136/150\n",
      "2156/2156 - 3s - loss: 0.0443 - val_loss: 0.0436\n",
      "Epoch 137/150\n",
      "2156/2156 - 3s - loss: 0.0444 - val_loss: 0.0437\n",
      "Epoch 138/150\n",
      "2156/2156 - 3s - loss: 0.0444 - val_loss: 0.0438\n",
      "Epoch 139/150\n",
      "2156/2156 - 3s - loss: 0.0444 - val_loss: 0.0436\n",
      "Epoch 140/150\n",
      "2156/2156 - 3s - loss: 0.0443 - val_loss: 0.0435\n",
      "Epoch 141/150\n",
      "2156/2156 - 3s - loss: 0.0443 - val_loss: 0.0436\n",
      "Epoch 142/150\n",
      "2156/2156 - 3s - loss: 0.0445 - val_loss: 0.0437\n",
      "Epoch 143/150\n",
      "2156/2156 - 3s - loss: 0.0444 - val_loss: 0.0435\n",
      "Epoch 144/150\n",
      "2156/2156 - 3s - loss: 0.0443 - val_loss: 0.0437\n",
      "Epoch 145/150\n",
      "2156/2156 - 3s - loss: 0.0443 - val_loss: 0.0435\n",
      "Epoch 146/150\n",
      "2156/2156 - 3s - loss: 0.0443 - val_loss: 0.0436\n",
      "Epoch 147/150\n",
      "2156/2156 - 3s - loss: 0.0443 - val_loss: 0.0436\n",
      "Epoch 148/150\n",
      "2156/2156 - 3s - loss: 0.0443 - val_loss: 0.0436\n",
      "Epoch 149/150\n",
      "2156/2156 - 3s - loss: 0.0443 - val_loss: 0.0437\n",
      "Epoch 150/150\n",
      "2156/2156 - 3s - loss: 0.0444 - val_loss: 0.0436\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential # import keras sequential library\n",
    "from tensorflow.keras.layers import Dense, LSTM # import keras dense and lstm layers\n",
    "from sklearn.model_selection import train_test_split # import train and test library\n",
    "from sklearn.metrics import mean_squared_error # import metrics\n",
    "import matplotlib.pyplot as plt # for visualization purpose\n",
    "\n",
    "x = X.reshape(X.shape[0], X.shape[1], 1) # As lstm need three dimensions, so add one extra dimension\n",
    "print(\"x:\", x.shape, \"y:\", Y.shape) # print shape\n",
    " \n",
    "in_dim = (x.shape[1], x.shape[2]) # save in input dimension\n",
    "out_dim = Y.shape[1] # save in out_dim variable\n",
    "print(in_dim) # print \n",
    "print(out_dim) # print \n",
    "\n",
    "xtrain, xtest, ytrain, ytest=train_test_split(x, Y, test_size=0.2) # call train_test_split function\n",
    "print(\"xtrain:\", xtrain.shape, \"ytrian:\", ytrain.shape) # print \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential() # call sequential constructot\n",
    "model.add(LSTM(32, input_shape=in_dim,return_sequences=True)) # configure lstm layers with 32 hidden neurons\n",
    "model.add(LSTM(64)) # 2nd layer is 64 hidden neurons\n",
    "model.add(Dense(out_dim,activation='linear')) # fully connected layer with 5 variable\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error') # use optimizer adam and loss is mse\n",
    "model.summary() # overall summary \n",
    "\n",
    "history=model.fit(xtrain, ytrain, validation_split=0.1,epochs=150, batch_size=32, verbose=2,shuffle=True) # train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZxU1Z338c+vqvcdmmZtEBBQWQQFFSdqNI4EYyImMUpc4iROTDJxJnkmZqJPJk5iZp4kM5lsE7OYqDHGXbOQCQZ3zeICKCqgSINoNyA00DS9b/V7/ji3m+q9Gyi6le/79apXV9177q1Tt6vu955zN3N3REREBio21BUQEZG3FwWHiIgMioJDREQGRcEhIiKDouAQEZFBUXCIiMigKDhEUsjMfmFm/z7AslvM7G8Pdj4iqabgEBGRQVFwiIjIoCg45IgXdRF90cxeMrM6M7vZzMaY2YNmVmNmj5jZiKTy55vZOjPba2ZPmNlxSeNOMLPno+nuAbK6vNf7zWxNNO1fzez4A6zzJ82szMz2mNkyMxsfDTcz+66Z7TSz6ugzzY7Gvc/M1kd122pm1xzQApMjnoJDJPgwcA4wA/gA8CDwf4FRhN/JPwGY2QzgLuDzQAmwHPi9mWWYWQbwW+B2YCRwXzRfomlPBG4BPgUUAz8FlplZ5mAqambvAb4BXASMA94A7o5GLwLOiD5HEXAxsDsadzPwKXfPB2YDjw3mfUXaKThEgv9x9x3uvhX4E/Csu7/g7k3Ab4ATonIXA39w94fdvQX4NpAN/A2wEEgHvufuLe5+P7Ay6T0+CfzU3Z919zZ3vw1oiqYbjEuBW9z9+ah+1wGnmtlkoAXIB44FzN1fcfft0XQtwEwzK3D3Knd/fpDvKwIoOETa7Uh63tDD67zo+XjCFj4A7p4AyoEJ0bit3vnKoW8kPT8K+ELUTbXXzPYCE6PpBqNrHWoJrYoJ7v4Y8EPgRmCHmd1kZgVR0Q8D7wPeMLMnzezUQb6vCKDgEBmsbYQAAMI+BcLKfyuwHZgQDWs3Kel5OfAf7l6U9Mhx97sOsg65hK6vrQDu/gN3nw/MInRZfTEavtLdlwCjCV1q9w7yfUUABYfIYN0LnGdmZ5tZOvAFQnfTX4GngVbgn8wszcw+BJycNO3PgE+b2SnRTuxcMzvPzPIHWYc7gY+b2bxo/8j/I3StbTGzk6L5pwN1QCPQFu2DudTMCqMutn1A20EsBzmCKThEBsHdNwCXAf8D7CLsSP+Auze7ezPwIeDvgCrC/pBfJ027irCf44fR+LKo7GDr8CjwFeABQivnaGBpNLqAEFBVhO6s3YT9MACXA1vMbB/w6ehziAya6UZOIiIyGGpxiIjIoCg4RERkUBQcIiIyKAoOEREZlLShrsDhMGrUKJ88efJQV0NE5G1l9erVu9y9pOvwIyI4Jk+ezKpVq4a6GiIibytm9kZPw9VVJSIig6LgEBGRQVFwiIjIoBwR+zh60tLSQkVFBY2NjUNdlZTKysqitLSU9PT0oa6KiLxDHLHBUVFRQX5+PpMnT6bzxUzfOdyd3bt3U1FRwZQpU4a6OiLyDnHEdlU1NjZSXFz8jg0NADOjuLj4Hd+qEpHD64gNDuAdHRrtjoTPKCKH1xEdHP3ZVdvE3vrmoa6GiMiwouDow57aZqobWlIy77179/KjH/1o0NO9733vY+/evSmokYjIwCg4+mKQqtuV9BYcbW1935Rt+fLlFBUVpaZSIiIDkNLgMLPFZrbBzMrM7Noexmea2T3R+GfNbHI0PN3MbjOzl83sFTO7bqDzfLu49tpr2bRpE/PmzeOkk07irLPO4pJLLmHOnDkAXHDBBcyfP59Zs2Zx0003dUw3efJkdu3axZYtWzjuuOP45Cc/yaxZs1i0aBENDQ1D9XFE5AiSssNxzSwO3AicA1QAK81smbuvTyp2JVDl7tPMbCnwLcLtNj8CZLr7HDPLAdab2V1A+QDmOWhf+/061m/b1214Q0sbBmSlxwc9z5njC/i3D8zqdfw3v/lN1q5dy5o1a3jiiSc477zzWLt2bcdhs7fccgsjR46koaGBk046iQ9/+MMUFxd3msfGjRu56667+NnPfsZFF13EAw88wGWX6W6gIpJaqWxxnAyUufvm6F7MdwNLupRZAtwWPb8fONvCYUAO5JpZGpANNAP7BjjPt6WTTz6507kWP/jBD5g7dy4LFy6kvLycjRs3dptmypQpzJs3D4D58+ezZcuWw1VdETmCpfIEwAmEFkK7CuCU3sq4e6uZVQPFhBBZAmwHcoD/4+57zGwg8wTAzK4CrgKYNGlSnxXtrWVQtrOWmMHUkrw+pz8UcnNzO54/8cQTPPLIIzz99NPk5ORw5pln9nguRmZmZsfzeDyurioROSxS2eLo6QSCrruaeytzMtAGjAemAF8ws6kDnGcY6H6Tuy9w9wUlJd0uJz8gqTwDIj8/n5qamh7HVVdXM2LECHJycnj11Vd55plnUlgTEZHBSWWLowKYmPS6FNjWS5mKqFuqENgDXAL80d1bgJ1m9hdgAaG10d88D6kUHVRFcXEx73rXu5g9ezbZ2dmMGTOmY9zixYv5yU9+wvHHH88xxxzDwoULU1QLEZHBS2VwrASmm9kUYCuwlBAIyZYBVwBPAxcCj7m7m9mbwHvM7FeErqqFwPeA9QOY56HTvrclRe68884eh2dmZvLggw/2OK59P8aoUaNYu3Ztx/BrrrnmkNdPRKQnKQuOaJ/F1cAKIA7c4u7rzOwGYJW7LwNuBm43szJCS2NpNPmNwK3AWsLq+1Z3fwmgp3mm6jMYkEjVzEVE3qZSenVcd18OLO8y7Pqk542EQ2+7Tlfb0/De5ikiIoePzhzvw/4jg0VEpJ2Cow9G6i45IiLydqXg6IdyQ0SkMwVHH3QrCxGR7hQc/TjcV8cdiO9973vU19cf4hqJiAyMgmOIKDhE5O0qpYfjvt1ZCs8ATL6s+jnnnMPo0aO59957aWpq4oMf/CBf+9rXqKur46KLLqKiooK2tja+8pWvsGPHDrZt28ZZZ53FqFGjePzxx1NSPxGR3ig4AB68Ft56udvg0a1tJBIOGQewmMbOgXO/2evo5MuqP/TQQ9x///0899xzuDvnn38+Tz31FJWVlYwfP54//OEPQLiGVWFhId/5znd4/PHHGTVq1ODrJSJykNRVNQw89NBDPPTQQ5xwwgmceOKJvPrqq2zcuJE5c+bwyCOP8KUvfYk//elPFBYWDnVVRUTU4gB6bRns2lNPTVMrx40rSOnbuzvXXXcdn/rUp7qNW716NcuXL+e6665j0aJFXH/99T3MQUTk8FGLoy8pPBw3+bLq733ve7nllluora0FYOvWrezcuZNt27aRk5PDZZddxjXXXMPzzz/fbVoRkcNNLY5+pOpw3OTLqp977rlccsklnHrqqQDk5eXxq1/9irKyMr74xS8Si8VIT0/nxz/+MQBXXXUV5557LuPGjdPOcRE57MyPgGtqLFiwwFetWtVp2CuvvMJxxx3X53Rbqxqobmhm5vi3976FgXxWEZGuzGy1uy/oOlxdVX0w0yVHRES6UnD0R8khItLJER0cA+mme7vnxpHQFSkih9cRGxxZWVns3r27zxXr2/0ih+7O7t27ycrKGuqqiMg7SEqPqjKzxcD3Cbd5/bm7f7PL+Ezgl8B8YDdwsbtvMbNLgS8mFT0eONHd15jZE8A4oCEat8jddw62bqWlpVRUVFBZWdlrmeqGFmqbWonvyx7s7IeNrKwsSktLh7oaIvIOkrLgMLM44d7h5wAVwEozW+bu65OKXQlUufs0M1sKfIsQHncAd0TzmQP8zt3XJE13qbt3PkxqkNLT05kyZUqfZb69YgM/eqKczd8472DeSkTkHSWVXVUnA2Xuvtndm4G7gSVdyiwBboue3w+cbdatg+ijwF0prGevYgYJ7SIQEekklcExAShPel0RDeuxjLu3AtVAcZcyF9M9OG41szVm9pUeggYAM7vKzFaZ2aq+uqP6EouFWSeUHiIiHVIZHD2t0LuugfssY2anAPXuvjZp/KXuPgc4PXpc3tObu/tN7r7A3ReUlJQMruaReJRJbToySUSkQyqDowKYmPS6FNjWWxkzSwMKgT1J45fSpbXh7lujvzXAnYQusZRob3G0qcUhItIhlcGxEphuZlPMLIMQAsu6lFkGXBE9vxB4zKPjY80sBnyEsG+EaFiamY2KnqcD7wfWkiLxKDjU4BAR2S9lR1W5e6uZXQ2sIByOe4u7rzOzG4BV7r4MuBm43czKCC2NpUmzOAOocPfNScMygRVRaMSBR4CfpeozRLmhrioRkSQpPY/D3ZcDy7sMuz7peSOhVdHTtE8AC7sMqyOc83FYxExdVSIiXR2xZ44PRFxHVYmIdKPg6EN7cKirSkRkPwVHH9pPEUkoOEREOig4+tB+HkciMcQVEREZRhQcfYhHS0ddVSIi+yk4+hAz7RwXEelKwdGHuM4cFxHpRsHRh5h2jouIdKPg6EPH1XEVHCIiHRQcfei4Oq6OqhIR6aDg6EPHUVXaxyEi0kHB0Qft4xAR6U7B0QcFh4hIdwqOPuhwXBGR7hQcfdBRVSIi3Sk4+qCjqkREulNw9KH9DoBqcYiI7Kfg6ENMN3ISEekmpcFhZovNbIOZlZnZtT2MzzSze6Lxz5rZ5Gj4pWa2JumRMLN50bj5ZvZyNM0PrP2mGSmgGzmJiHSXsuAwszhwI3AuMBP4qJnN7FLsSqDK3acB3wW+BeDud7j7PHefB1wObHH3NdE0PwauAqZHj8Wp+gy657iISHepbHGcDJS5+2Z3bwbuBpZ0KbMEuC16fj9wdg8tiI8CdwGY2TigwN2fdncHfglckKoPENdRVSIi3aQyOCYA5UmvK6JhPZZx91agGijuUuZiouCIylf0M08AzOwqM1tlZqsqKysP6AN07BzXUVUiIh1SGRw97XvouuneZxkzOwWod/e1g5hnGOh+k7svcPcFJSUlA6lvNx1dVWpxiIh0SGVwVAATk16XAtt6K2NmaUAhsCdp/FL2tzbay5f2M89DJq6jqkREukllcKwEppvZFDPLIITAsi5llgFXRM8vBB6L9l1gZjHgI4R9IwC4+3agxswWRvtCPgb8LlUfQEdViYh0l5aqGbt7q5ldDawA4sAt7r7OzG4AVrn7MuBm4HYzKyO0NJYmzeIMoMLdN3eZ9WeAXwDZwIPRIyV0VJWISHcpCw4Ad18OLO8y7Pqk542EVkVP0z4BLOxh+Cpg9iGtaC/ad46rwSEisp/OHO+Dro4rItKdgqMPOqpKRKQ7BUcfdFSViEh3Co4+7L8D4BBXRERkGFFw9CEWLR11VYmI7Kfg6EP7jZzUVSUisp+Cow86qkpEpDsFRx90z3ERke4UHH3Yv3NcwSEi0k7B0Yd4xyVHhrgiIiLDiIKjD+1HVanFISKyn4KjD3Fd5FBEpBsFRx90VJWISHcKjj603/7c1VUlItJBwdGPeMx05riISBIFRz/iZjqqSkQkiYKjH7GYjqoSEUmW0uAws8VmtsHMyszs2h7GZ5rZPdH4Z81sctK4483saTNbZ2Yvm1lWNPyJaJ5rosfoVH6GmJmuVSUikiRlt441szhwI3AOUAGsNLNl7r4+qdiVQJW7TzOzpcC3gIvNLA34FXC5u79oZsVAS9J0l0a3kE25uGkfh4hIslS2OE4Gytx9s7s3A3cDS7qUWQLcFj2/HzjbwqFMi4CX3P1FAHff7e5tKaxrr2IxtThERJKlMjgmAOVJryuiYT2WcfdWoBooBmYAbmYrzOx5M/uXLtPdGnVTfcXaj5ntwsyuMrNVZraqsrLygD+EjqoSEekslcHR0wq96xq4tzJpwGnApdHfD5rZ2dH4S919DnB69Li8pzd395vcfYG7LygpKTmQ+gNhH4eOqhIR2S+VwVEBTEx6XQps661MtF+jENgTDX/S3Xe5ez2wHDgRwN23Rn9rgDsJXWIpEzOdACgikiyVwbESmG5mU8wsA1gKLOtSZhlwRfT8QuAxD2vpFcDxZpYTBcq7gfVmlmZmowDMLB14P7A2hZ8hdFVpH4eISIeUHVXl7q1mdjUhBOLALe6+zsxuAFa5+zLgZuB2MysjtDSWRtNWmdl3COHjwHJ3/4OZ5QIrotCIA48AP0vVZ4Coq0otDhGRDikLDgB3X07oZkoedn3S80bgI71M+yvCIbnJw+qA+Ye+pr2L66gqEZFOdOZ4P8JRVUNdCxGR4UPB0Q8zXXJERCSZgqMfcV1yRESkkwEFh5l9zswKLLg5OilvUaorNxzoqCoRkc4G2uL4hLvvI1wKpAT4OPDNlNVqGImZqatKRCTJQIOj/Qzv9wG3RteQ6vFSH+804bLqQ10LEZHhY6DBsdrMHiIExwozyweOiAtxhBs5KTlERNoN9DyOK4F5wGZ3rzezkYTuqne8WExdVSIiyQba4jgV2ODue83sMuBfCVeyfcdTi0NEpLOBBsePgXozmwv8C/AG8MuU1WoYiemoKhGRTgYaHK3RxQeXAN939+8D+amr1vARro471LUQERk+BrqPo8bMriPc++L06Law6amr1vARjxktuiGHiEiHgbY4LgaaCOdzvEW4c99/paxWw0hM+zhERDoZUHBEYXEHUGhm7wca3f2I2McR11FVIiKdDPSSIxcBzxEugX4R8KyZXZjKig0XanGIiHQ20H0cXwZOcvedAGZWQriJ0v2pqthwES45MtS1EBEZPga6jyPWHhqR3YOY9m0tHkNXxxURSTLQFscfzWwFcFf0+mK63NnvnSrcyEnBISLSbqA7x78I3AQcD8wFbnL3L/U3nZktNrMNZlZmZtf2MD7TzO6Jxj9rZpOTxh1vZk+b2Toze9nMsqLh86PXZWb2AzNL6cUWY7ofh4hIJwO+57i7PwA8MNDy0bkeNwLnABXASjNb5u7rk4pdCVS5+zQzWwp8C7jYzNII9xu/3N1fNLNioCWa5sfAVcAzhFbPYuDBgdZrsGKmFoeISLI+WxxmVmNm+3p41JjZvn7mfTJQ5u6b3b0ZuJtw5nmyJcBt0fP7gbOjFsQi4KXo8u24+253bzOzcUCBuz8dncn+S+CCQX3iQdLhuCIinfXZ4nD3g7msyASgPOl1BXBKb2XcvdXMqoFiYAbg0X6VEuBud//PqHxFl3lO6OnNzewqQsuESZMmHfCHCF1VBzy5iMg7zoC7qg5AT/seum6691YmDTgNOAmoBx41s9VAT62cHpsD7n4TYb8MCxYsOOAmQzyGzuMQEUmSykNqK4CJSa9LgW29lYn2axQCe6LhT7r7LnevJ+zLODEaXtrPPA8pHVUlItJZKoNjJTDdzKaYWQawFFjWpcwy4Iro+YXAY9G+ixXA8WaWEwXKu4H17r6dcMHFhdG+kI8Bv0vhZ8DMcAWHiEiHlHVVRfssriaEQBy4xd3XmdkNwCp3XwbcDNxuZmWElsbSaNoqM/sOIXwcWO7uf4hm/RngF0A24WiqlB1RBbqRk4hIV6ncx4G7L6fLiYLufn3S80bC9a96mvZXhENyuw5fBcw+tDXtXVw3chIR6eSIuGzIwdC1qkREOlNw9CNmOqpKRCSZgqMfOgFQRKQzBUc/YgoOEZFOFBz90FFVIiKdKTj6EVoc6FwOEZGIgqMfseiiKGp0iIgECo5+xKPbfWg/h4hIoODoRyxqcmg/h4hIoODoRzymFoeISDIFRz/au6rU4hARCRQc/TDtHBcR6UTB0Y+Oriolh4gIoODoV3tw6GZOIiKBgqMfMVOLQ0QkmYKjH+3BoRaHiEig4OhHPFpCanCIiAQpDQ4zW2xmG8yszMyu7WF8ppndE41/1swmR8Mnm1mDma2JHj9JmuaJaJ7t40an8jOoq0pEpLOU3TrWzOLAjcA5QAWw0syWufv6pGJXAlXuPs3MlgLfAi6Oxm1y93m9zP7S6BayKRfXmeMiIp2kssVxMlDm7pvdvRm4G1jSpcwS4Lbo+f3A2WbtZ04MDzqqSkSks1QGxwSgPOl1RTSsxzLu3gpUA8XRuClm9oKZPWlmp3eZ7taom+orvQWNmV1lZqvMbFVlZeUBfwhTV5WISCepDI6eVuhd1769ldkOTHL3E4B/Bu40s4Jo/KXuPgc4PXpc3tObu/tN7r7A3ReUlJQc0AeA5KvjHvAsRETeUVIZHBXAxKTXpcC23sqYWRpQCOxx9yZ33w3g7quBTcCM6PXW6G8NcCehSyxl2o+q0j4OEZEglcGxEphuZlPMLANYCizrUmYZcEX0/ELgMXd3MyuJdq5jZlOB6cBmM0szs1HR8HTg/cDaFH6G/UdVaR+HiAiQwqOq3L3VzK4GVgBx4BZ3X2dmNwCr3H0ZcDNwu5mVAXsI4QJwBnCDmbUCbcCn3X2PmeUCK6LQiAOPAD9L1WcAHVUlItJVyoIDwN2XA8u7DLs+6Xkj8JEepnsAeKCH4XXA/ENf096pxSEi0pnOHO9HTDdyEhHpRMHRj/03chriioiIDBMKjn7EdFSViEgnCo5+aB+HiEhnCo5+xLWPQ0SkEwVHPzrux6GuKhERQMHRL7U4REQ6U3D0Q0dViYh0puDoR/u1d9VVJSISKDj60d5V5eqqEhEBFBz90o2cREQ6U3D0Q0dViYh0puDoh46qEhHpTMHRjyg3SOioKhERQMHRr46uKrU4REQABUe/OrqqtI9DRARQcPRLR1WJiHSW0uAws8VmtsHMyszs2h7GZ5rZPdH4Z81scjR8spk1mNma6PGTpGnmm9nL0TQ/MGs/RS9VnyH8VYtDRCRIWXCYWRy4ETgXmAl81Mxmdil2JVDl7tOA7wLfShq3yd3nRY9PJw3/MXAVMD16LE7VZ4D9lxxRboiIBKlscZwMlLn7ZndvBu4GlnQpswS4LXp+P3B2Xy0IMxsHFLj70x5O5f4lcMGhr/p+HV1VSg4RESC1wTEBKE96XREN67GMu7cC1UBxNG6Kmb1gZk+a2elJ5Sv6mScAZnaVma0ys1WVlZUH/CF0z3ERkc5SGRw9tRy6rn17K7MdmOTuJwD/DNxpZgUDnGcY6H6Tuy9w9wUlJSWDqHZncZ05LiLSSSqDowKYmPS6FNjWWxkzSwMKgT3u3uTuuwHcfTWwCZgRlS/tZ56HlM7jEBHpLJXBsRKYbmZTzCwDWAos61JmGXBF9PxC4DF3dzMriXauY2ZTCTvBN7v7dqDGzBZG+0I+BvwuhZ+BWLSElBsiIkFaqmbs7q1mdjWwAogDt7j7OjO7AVjl7suAm4HbzawM2EMIF4AzgBvMrBVoAz7t7nuicZ8BfgFkAw9Gj5RRV5WISGcpCw4Ad18OLO8y7Pqk543AR3qY7gHggV7muQqYfWhr2jsdVSUi0pnOHO+HmWGmo6pERNopOAYgZqYWh4hIRMExAHEznTkuIhJRcAxALKauKhGRdgqOAYirq0pEpIOCYwC0j0NEZD8FxwDEYoarq0pEBFBwDEg8ZrrkiIhIRMExAKGraqhrISIyPCg4BiAe0x0ARUTaKTgGIGbqqhIRaafgGICYmc7jEBGJKDgGIB4zdVWJiEQUHAMQjqoa6lqIiAwPCo4BMNPOcRGRdgqOAdAlR0RE9lNwDEA8pp3jIiLtFBwD0H5UVVvCadWZgCJyhEvprWPNbDHwfcI9x3/u7t/sMj4T+CUwH9gNXOzuW5LGTwLWA191929Hw7YANYR7kbe6+4JUfgYILY4/bdzFcV/5I23ujCvMYtroPBYcNYKzjh3NrPGFqa6CiMiwkbLgMLM4cCNwDlABrDSzZe6+PqnYlUCVu08zs6XAt4CLk8Z/F3iwh9mf5e67UlT1bj54wgT+umkXR4/OIy1mbK1qYP32fXx7QyXffug1Fs0cwzXvPYYZY/IB2NfYQsWeBo4bl4+ZHfT7v1xRzW9e2Molp0xk2uj8g56fiMjBSGWL42SgzN03A5jZ3cASQgui3RLgq9Hz+4Efmpm5u5vZBcBmoC6FdRyQT5w2hU+cNqXb8Kq6Zm5/5g1+9tRmzv3+n/j430zm2HEFfPPBV9hV28xRxTl86IRSPnTiBCaOzKGhuY2VW/bw+xe38fLWauaWFnHCpCJ21zWzvbqBtgSkxYzpY/KYXJzLum37eOzVHazcUgXA71/axn2fOpXJo3L7rG9rW4LfrtlGPAbnzh5HVno8JculJxvequG/Vmzgc2dPZ06pWmIi70SWqsuFm9mFwGJ3//vo9eXAKe5+dVKZtVGZiuj1JuAUoAF4hNBauQaoTeqqeh2oAhz4qbvf1Mv7XwVcBTBp0qT5b7zxRko+J4QA+c8VG7jruTcBmFtayIXzS3lw7Vv8ddNuAI4qzqGiqoG2hJOfmcbxEwt5qaKamsZWAEbkpJMWj9HU0sa+aBjAjDF5fGT+ROZPHsGVv1hJTkYaF580kebWBFt211G2s5bSEdmcMaOE8YXZNLS08eMnNrF++z4ACrPTWTh1JCNzMynOzWBEbgY7axp5ZvMe9tY3M74wm2mj8zj16GJmjMmjsSVBZU0TZTtraWhp46xjRjN5VA6Pb6jkxfK9ZKTFiBnsqmmmqr6Z9LQYBVnpvHfWGEbkZPB3tz5HVX0LRTnp3HPVqRwzNp+WtgR/KdvFU6/tYtb4AhbPHktuZthmqWtq5S9lu8jNTGNOaSHNrQnKdtaSFjMmjsyhODeDtPjgdsU1NLfx+q46Nu+qpaUtwTkzx5KXOfBtpNd31fHA6gqa2xJkp8f58ImlTCrO6bFsXVMr9c1txAyKcjKIxwbWwmxsaaOpNUFhdnqn4btqm1j5+h7S4jHGF2WpG3SAWtoSxM2IDXD5y8CY2eqedgekMjg+Ary3S3Cc7O7/mFRmXVQmOThOBq4DnnP3e83sq3QOjvHuvs3MRgMPA//o7k/1VZcFCxb4qlWrDv2H7GJN+V42V9ayZN6EjhVIRVU9v3l+K2u3VXPMmHzmTiziXdNGkZUep7Utwda9DZTkZ5KTEVZs7s5b+xrZXFnHjDH5lORndsx/7dZq/u7W59hV2wxA6Yiw0t9cWcebe+o7yo0tyOL6D8ykKCedu54rZ8Nb+9hT10xVfQttCSc9bsybWMSYgiy27W1gw1s11DW39fq5zDKBSNMAABfnSURBVMAdstJjJBLQmkgwKi+TETkZtCRC0LQH4ISibL754Tlcc9+LtLQ5pSOy2VxZR21TaziRMuHkZMSZNDKHrPQ467fvo7m17wMOMuIxRuZmML4oi5gZ26sbaWpNUJCdRn5WOgVZaaTFjOqGFnbsa2Lr3oZO0+dmxDnzmNG0JZyaphZqG1upbWqlqTVBzIw5pYXMLS2ksSXBhrdqeHDtdsyM9LjR1JogPR7jU2dMJS0W46WKvWSmxyjOzWTdtmrWlO/tuB99RlqMo0vymD46PEblZ5IWM9LjMdLjMVraEuxrbGH1G1U8sn4Hdc1tTByZzYKjRnL+3PHsqm3i6/+7vtOGw+JZY/mHs45mb30LO/Y1Eo8ZafEY6dHftLiREY+RFjNqm1r57ZptPPbKDkbmZXDUyFziMcOB6aPzmDuxiIbmVt7cU8+bexoo31OPGeRnpTN1VC5zJxYycUQOhdnpFGanU5CdTlNLgh01jbxV3ciOfY3UR9+T+uY2dtU2Ub6nng07amhuTTC3tIhR+Rm8sr2G2sZWzjymhFkTCtlcWUtlTRNHl+QxY0w+YwszGZmbibcfcJJwYmaMKcjs6Nqtbmjh4fU7ePzVnVQ3tNDSluDo0XnMnzSCY8bmM3FkDm0JZ3t1A/etquDeVeWkx2MsnDqSU6cWc+rRo5g+Oq8jSBIJZ29DCztrGsnPSmdCUTaNLW3cu6qcrVUNLJo1luNLC9mxr5GNO2pZ/UYVu+uaOG1aCaceXUxRdnq3UGptS1Dd0EJ9cxtjCrLISIt1/IYPRRd1+7ljQxmGQxEcpxJ2ar83en0dgLt/I6nMiqjM02aWBrwFlABPAROjYkVAArje3X/Y5T2+SlKo9OaQBIc7rPi/YDF4738c3LwOog6J1bfhRUcRm3I6Ft+/FV2+p5699S0ATBudR3ZG9+6pRMLZ19hCZlq80/iWtgQvVVRTUVVPTkYaI3LSObokDwcefWUHb+6p590zSjhh0oiwImr/YSQSEIvR1NrGo6/s5K+bdvEPZ05jfFE2ZTtr+PJv1pKRFmNycS5nzCjh9OmjWLu1mmUvbmN7dSO1ja0cOy6fc2aOobXNeXlrNVnpcaaNziORcMqrwmeqa25ld20zW6sacJzxhdlkpsepaWxhX2Mr+xpCIBZmpzMqL4Mpo/KYWpLL0SV5NLS0ceezb/LM5t3kZsbJz0onPyuN3Iw0MtNiNLUmWP1GFW/tawRgZG4GH5lfyt+fPpWS/Ezeqm7k639Yzx9e2o4ZTCvJo82dyn1NTC3J5fTpJYwpzKKtLcG26kY27qhh485aKqoaui3/diNy0lk8eywTR+awbus+/ly2i+qG8L9bcNQIvnTusWSlxXliw05+9MQmGlp6D/WuCrPTWTxrLA0tbby5px53J+Hw2o4amqKAjseMCUXZTByZjRECt72FORjZ6XHGF2Vx7NgC4jHjhfIqqupamDmugIy0GM++vpuWNg/hlJnWKRB7UpybwczxBWzb28CW3fW0JcKBKOOLsjFCN2hNU/d5pMeN9x8/nox4jL9u3kX5nrDs02LG6PxMHKisaaI16VysY8fmU93Qwvbqxo4NmvYNpPZllJMe7/R+uRlxcjPTyEyPUV3f0unzxAxK8jNpaG6jrrmNo4pzOGZMPo0tbVTVt1A6IpvjxhUAUJs0z1e37+OF8r2Myc/irGNHM7k4hzb3jo2L5rYEM8bkc8yYfI4Zm8+ovEyaWtuImVGQnc6ovEwmjsxmZE4Gja0JGprbaGxpY2dNE+u3VbOpso5/+8DMAw6yoQiONOA14GxgK7ASuMTd1yWV+Swwx90/He0c/5C7X9RlPl8lCgczywVi7l4TPX8YuMHd/9hXXQ5JcDx/OyyLetkuvR+mnwNlj8Iry6ClAfJGw0l/DyMmd5+2bhdUl8O4eWHzfePD8MLt8N5vQOGEgdfhpfvg138fnhdMgL/9Khx/UV9TpM6zP4UnvgGX3AcTT+q9nDu8/iRMmA+Zw2DHvjusuRPW3g8fvAnySnB39ta3kJuZ1rHV2NWWXXWMyM3o1rXUofI1qHw1fBemnkl9ZjHVDS20tjktbQlaE048ZhRkpXd0S7Zrbk3w1GuVNLUmOHf22E5bmNurG3hm827GF2YzrjCbhDutiQQtbR7mnUjQ0hrmbwYnThrR4z6t5tYEG3fWUJCVzrjCrG7df61tCTZV1vHWvkb2NbRQHT0y02KMKchiTEEWYwuyyM2MY2ZkpsU6uht7s6+xhTd313N0SR5Z6TEqa0MXaGVNE3vqmonHLLSgYqFl92J5Na++tY8JRdkcOzafs44dzbyJRR0rvUTC2VRZy6bKOt7cU0d61BJdOLWYMQVZHe9bvqeeZzbv5vVd4fMYxuiCTEbnZ1KSn8n2vY088soO4jHjs2dN4/jSQh5at4PXd9UxYUQ2k4tD6ysjHuOF8r2seXMvNU2t1DW1UtvYSlNrG4XZ6RTlZFCUk052epxt1Y1s39tAbmYaWelxNlfWUrazluyMOEU56byxu75jYyItZh1X3J5cnMOJk0ZQUdXAyi17OsKtMDuds48bTVF2Bq/tqOHVt2rYVdvU5/LuycjcDB77wrspyskY9LQwBMERven7gO8RDse9xd3/w8xuAFa5+zIzywJuB04A9gBL23emJ83jq+wPjqnAb6JRacCd7t7v5v8BB8d9H4f0bJh6Jvz+c2HlV7szrBxO+zwsvwYy8iG7CPZtA2+D2RfCon+H/DFQ/hz89Qew4UFItMLk08M8/vJ9wKF4Onx8eQgdgLrdsP0FOOo0SM/qXJfGffDDkyB/LLzrc/DMj6BiJZzyaTjn65CW9MVoroPnboKsQjjhcoinQ/2e8MgugtYm2Psm5JbAqGmd3+fNZ2HNHdBcG+bTXBfmc84NUHx0KPPCHfC7fwitr8JS+NSfwny7aq6D310N634NBaXw/u/CjEXdy1W9EUIlZ2Tn4YkE7FwHm58My2jWhyA+iOM52lqgpT7UH6ChCn7/eVj/2/B63mVwwY3huXsI9QOx9tfw60+G/zHAyKPhqicgq6DzZ9nxMmx6HEYcBbM+OLj3cAdPQOzwHejQL/fwiEUhlEhAayNk9Lw/KCXKHgnLe2T3g1eGm7qouzYzLdZjC6C2qZWaxtDyHJWXSXqXcN9d20R1QwtZ6XHaEk511PVWvqeBvfUtZGfEyE6Pk5Uepygng1njCxhXmHVQ3WZDEhzDxQEFhzv8/p9g7W+guQayR8Bn/hpWcrcuDmWmnAFL74LMvBAcz/4EnvlJWOmXnhS+1DnFMPejkD8O/vTf0LAH5lwEc5fCPZdB3hgYNzcM3/KXED5FR4UV9Yijwso3IzesrFf+HP7+USidH1aKD18fAiSzMLSARh8L8Qx47mehhQMhnApL4fWnwrw7MTjxY/DufwllXroPfvuZEJa5JeF9M/LCyrutBU75FOwug1f/AFPeDWdcA7edD8eeF1o/OcWw8xV46yXYszmsJHe9Bn/zj7DxobBFXjgJJpwIpQug5Niw9b/u1yGEJp4CMxbD9EUhFP/8Xah6fX91i6eFoJzybhg1ff/Oly1/gm0vhBVr+wq26vVQz6YaWPgZmHwG/O/noXYHnPVlqN8NT/8Qrnw4BNOfvwOTFoagPfY8SMukX+6w+lb4wxdg4kI495uwtxzuvRxmXgAX3hLKvfq/8MjXYPfG/dOe9s9w9vX7w6q1GVobILOgc4C1tcC634YNkMoNYZqF/7B/Zd0ukYC9b4QWb/v0bS1huW9YDqNnwawLoGB8VL4Ntq2Boon7N1z2vB5ax2ZhA2nPphDoE08JofjG02GD6LjzoXEv3Psx2LMFLvgRFE2C+z8RPuNFt8PUd3euX2tTqEvFqrABMm4ujD3+wMPaHZ76Njz+75CeC4v/H5x4xYHPD8Iy3LgC9m0Nv9mMPo5ebGkI/4+iSZ03eNyhZjvkje3+P2rXXB9+C7s3wZiZMGbWgdc5xRQcB9pV1dIQAqBoUviyAzwRrSDO++/uLYNdG8NW7Vsvw99cDad+dv8XsKEKtr8YVnxmYYX10JfDSiMtE6adDWNmw1P/Fb5YXZ34MTj/fzoP2/Q4vHx/+MLXVYZho2fCed8JP+5HvgZtTTBzSVhRN1ZDLC2E06ZHQ8sk0QrZI0N4HXUaLP1VCMp21VtDoLz+ZFjxTz8HFn09fK4/fxce+Wr3umbkhRXEe66H6X8bVhzP/xLe+AtUrIbqcAQa6bkhkGJp8NqDYbm1G39C6P6behZsex4e/0YIMQj1GzM7/Eh3l/Xw/vlw7PvA4vDinWHYyKPhwz8PwdVUE1pwjdWhVTL1rDCf6vIw72PfH55veyGEWkYeEK2Uxs8NQfTyvSHgpv1tWFm2b2k/9W147Osw/sTwP6kuh1EzQktx6lnw1H/C6l/A2DmQlh1CrGpLCPaMvBCQ0xeFDZL2jYBRM0K4b3ostFzn/x1MPi20IitWwtM3wq4NcNS74D3/Gr5bq24O75+RF1qQACOmhPlvXxPGWTxsANXugJ3rGZCjToO6naHO+WPDbyEjNyynvNFh+IIrw3e48lVIywrfxcbqaBn6/rrM/lDodk3PDhtf1RXh/5FoDV27U84I826uC8uh6g1oaw7/qzV3hBZ+3c6wYTRmdmjJFUwIw2p3hs+YWxJ+WzmjwoZM7Y7w/6/fHd6zuTbaQFofNnggrPhP/WwIVouHDZHqraFuNdthy59D66q97IxFMGEBvPArqHgurC/mXhICdOycUI/tL8G634QeiLakbqfxJ8AJl4XPgof/8caHw9/2XoPsEeH32tYCR58Vfjebnwghl1UYrWMMWupCmLc2hA2hKe8O3+UDbAUqOA7DUVWHTFtLCCv36EdTG77ox57X936C1iZoqg1fst62drravQleWwGVr4QWw5nX9by17R6Cr2t3knvYity9Mfw4Rh0D4+eFllRfW3+1O0NIjJ2zf4sXwoqj7NHwg516Vud5uIcf9pY/hyB5a21YKZ14eWipxDPCyssMYun7l0H5ytAqOeVTnbci1/02dDGdfT2cenVoqWx+Iux/em0FjJwaWo6xtP0r3kRreP+a7aEVeeZ1MO/Szl1oiQT88UthRVFYGlYecy/ZX8Yd/vo/4T3i6eGHXzwtdG3t2xY2LsqfDfU56rTQYpu+KHyu1bfCY/8B9V3Ofx0zB45ZHFqlDeG8H2YshgWfgKPPDiu+V34f5l25AUYfB8ecG1bs638HuaNh5vmhHu7huzBySphX+bNh5TlpYViZtW8oLL0zbEw9fH1onXzg++G7d9/fhXJjZoeVf6Il/G9mLgkrsupyePNpeOkeeP1PdAQJhPfNyAt1aN+46M3JV8Hib4XnL9y+f6XdLi1qOde+FcKmq8xCKBgX3q+lIXy3F3wihOHD14dA7lS+IHx/sorC/7T0pPD/2vZCaE0114bAmHdp+Hybn+z82SCE1+wPhdAfeXT4Lr1wO+xYC/HMsKw8EZbj1LNCMJQ/G6bNHxe+i+29CTnFoUehqSb6fnr4PYyYEn4Hb/41BO6X3ggbIQdAwfF2Cg45fFqbO+8fGgj30LIsmhi2lFOhfZ9U131QELqZKlaF8MwbHQKu/cCLut2w/jehRVQyIzV1a9gbAjR3VM/j3cMKayArq5aG0BJprgsr7ORgr94K5c+EIE7PCi2JEZMhPSd81p42cPZt23+wSkZeKNdcF7qBW+pCy61gQhjX1/6y9i6nhr0hdEYc1bkV3u1zNIaNrzFz9s+3thK2rg6hkDcaSo4LG1XxLgdYuIcW4Ev3hZbB9EVhX2j7/qxdG0MdRs8Mr3dvCi2K0bP63kBMtIUNrVHTey/TDwWHgkNEZFB6Cw5dHVdERAZFwSEiIoOi4BARkUFRcIiIyKAoOEREZFAUHCIiMigKDhERGRQFh4iIDMoRcQKgmVUCB3oLwFHAYbu/+QFSHQ/ecK8fqI6Hiuo4cEe5e0nXgUdEcBwMM1vV05mTw4nqePCGe/1AdTxUVMeDp64qEREZFAWHiIgMioKjfzcNdQUGQHU8eMO9fqA6Hiqq40HSPg4RERkUtThERGRQFBwiIjIoCo5emNliM9tgZmVmdu1Q1wfAzCaa2eNm9oqZrTOzz0XDR5rZw2a2Mfrbx63KDltd42b2gpn9b/R6ipk9G9XxHjMb5G33Dnn9iszsfjN7NVqepw635Whm/yf6P681s7vMLGuol6OZ3WJmO81sbdKwHpebBT+IfkMvmdmJQ1jH/4r+1y+Z2W/MrChp3HVRHTeY2XuHqo5J464xMzezUdHrIVmOfVFw9MDM4sCNwLnATOCjZjZzaGsFQCvwBXc/DlgIfDaq17XAo+4+HXg0ej3UPge8kvT6W8B3ozpWAVcOSa32+z7wR3c/FphLqOuwWY5mNgH4J2CBu88G4sBShn45/gJY3GVYb8vtXGB69LgK+PEQ1vFhYLa7Hw+8BlwHEP1+lgKzoml+FP3+h6KOmNlE4Bwg+YbrQ7Uce6Xg6NnJQJm7b3b3ZuBuYMkQ1wl33+7uz0fPawgruwmEut0WFbsNuGBoahiYWSlwHvDz6LUB7wHuj4oMaR3NrAA4A7gZwN2b3X0vw2w5AmlAtpmlATnAdoZ4Obr7U8CeLoN7W25LgF968AxQZGbjhqKO7v6Qu7dGL58BSpPqeLe7N7n760AZ4fd/2OsY+S7wL0DyUUtDshz7ouDo2QSgPOl1RTRs2DCzycAJwLPAGHffDiFcgNFDVzMAvkf48iei18XA3qQf7lAvz6lAJXBr1J32czPLZRgtR3ffCnybsOW5HagGVjO8lmO73pbbcP0dfQJ4MHo+bOpoZucDW939xS6jhk0d2yk4emY9DBs2xy2bWR7wAPB5d9831PVJZmbvB3a6++rkwT0UHcrlmQacCPzY3U8A6hge3Xsdov0ES4ApwHggl9Bl0dWw+V72YLj93zGzLxO6fO9oH9RDscNeRzPLAb4MXN/T6B6GDelyVHD0rAKYmPS6FNg2RHXpxMzSCaFxh7v/Ohq8o73pGv3dOVT1A94FnG9mWwhdfO8htECKoi4XGPrlWQFUuPuz0ev7CUEynJbj3wKvu3ulu7cAvwb+huG1HNv1ttyG1e/IzK4A3g9c6vtPYBsudTyasJHwYvTbKQWeN7OxDJ86dlBw9GwlMD06giWDsPNs2RDXqX1fwc3AK+7+naRRy4AroudXAL873HVr5+7XuXupu08mLLfH3P1S4HHgwqjYUNfxLaDczI6JBp0NrGcYLUdCF9VCM8uJ/u/tdRw2yzFJb8ttGfCx6KighUB1e5fW4WZmi4EvAee7e33SqGXAUjPLNLMphB3Qzx3u+rn7y+4+2t0nR7+dCuDE6Ls6bJZjB3fXo4cH8D7C0RebgC8PdX2iOp1GaKK+BKyJHu8j7EN4FNgY/R051HWN6nsm8L/R86mEH2QZcB+QOcR1mwesipblb4ERw205Al8DXgXWArcDmUO9HIG7CPtcWggrtyt7W26ELpYbo9/Qy4QjxIaqjmWE/QTtv5ufJJX/clTHDcC5Q1XHLuO3AKOGcjn29dAlR0REZFDUVSUiIoOi4BARkUFRcIiIyKAoOEREZFAUHCIiMigKDpFhzMzOtOgKwyLDhYJDREQGRcEhcgiY2WVm9pyZrTGzn1q4H0mtmf23mT1vZo+aWUlUdp6ZPZN0b4j2+1dMM7NHzOzFaJqjo9nn2f57h9wRnUkuMmQUHCIHycyOAy4G3uXu84A24FLChQmfd/cTgSeBf4sm+SXwJQ/3hng5afgdwI3uPpdwXar2y0qcAHyecG+YqYTrgYkMmbT+i4hIP84G5gMro8ZANuFCfwngnqjMr4Bfm1khUOTuT0bDbwPuM7N8YIK7/wbA3RsBovk95+4V0es1wGTgz6n/WCI9U3CIHDwDbnP36zoNNPtKl3J9Xd+nr+6npqTnbeh3K0NMXVUiB+9R4EIzGw0d9+A+ivD7ar+S7SXAn929Gqgys9Oj4ZcDT3q4r0qFmV0QzSMzukeDyLCjLReRg+Tu683sX4GHzCxGuOLpZwk3iJplZqsJd/C7OJrkCuAnUTBsBj4eDb8c+KmZ3RDN4yOH8WOIDJiujiuSImZW6+55Q10PkUNNXVUiIjIoanGIiMigqMUhIiKDouAQEZFBUXCIiMigKDhERGRQFBwiIjIo/x8byADtXLfuwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  20.458174,   55.60243 , 1013.111   ,   72.608154,  452.90356 ],\n",
       "       [  20.470823,   56.197086, 1013.1004  ,   72.58765 ,  452.3253  ],\n",
       "       [  19.881907,   54.68037 , 1012.9793  ,   73.62792 ,  454.17462 ],\n",
       "       ...,\n",
       "       [  18.907513,   52.524307, 1013.40485 ,   74.212814,  456.74854 ],\n",
       "       [  19.847376,   54.55671 , 1013.2332  ,   74.64525 ,  454.6828  ],\n",
       "       [  19.31717 ,   53.29331 , 1013.40704 ,   75.07038 ,  456.08585 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testpred = model.predict(xtest) # validate model on test data \n",
    "testpred = scaler.inverse_transform(testpred) # denormalized the data\n",
    "testpred # show the test prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60425654],\n",
       "       [0.65142364],\n",
       "       [0.6957147 ],\n",
       "       [0.46821973],\n",
       "       [0.84658884],\n",
       "       [0.47555637],\n",
       "       [0.80901131],\n",
       "       [0.73184969],\n",
       "       [0.32055749],\n",
       "       [0.34444998],\n",
       "       [0.41737183],\n",
       "       [0.63066202],\n",
       "       [0.87451401],\n",
       "       [0.7248961 ],\n",
       "       [0.67971578],\n",
       "       [0.64123877],\n",
       "       [0.1342454 ],\n",
       "       [0.23186547],\n",
       "       [0.29913068],\n",
       "       [0.38135956]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest[0] # for one row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_row = np.expand_dims(xtest[0], axis=0) #  add dummy dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  20.458174,   55.60243 , 1013.111   ,   72.60814 ,  452.90356 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testpred_row = model.predict(test_one_row) # test with one row \n",
    "testpred_row = scaler.inverse_transform(testpred_row) # denormalized the prediction\n",
    "testpred_row # show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
