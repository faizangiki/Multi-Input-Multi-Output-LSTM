{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>V</th>\n",
       "      <th>AP</th>\n",
       "      <th>RH</th>\n",
       "      <th>PE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.64</td>\n",
       "      <td>58.49</td>\n",
       "      <td>1011.40</td>\n",
       "      <td>74.20</td>\n",
       "      <td>445.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.74</td>\n",
       "      <td>56.90</td>\n",
       "      <td>1007.15</td>\n",
       "      <td>41.91</td>\n",
       "      <td>438.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.07</td>\n",
       "      <td>49.69</td>\n",
       "      <td>1007.22</td>\n",
       "      <td>76.79</td>\n",
       "      <td>453.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.80</td>\n",
       "      <td>40.66</td>\n",
       "      <td>1017.13</td>\n",
       "      <td>97.20</td>\n",
       "      <td>464.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.97</td>\n",
       "      <td>39.16</td>\n",
       "      <td>1016.05</td>\n",
       "      <td>84.60</td>\n",
       "      <td>470.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9562</th>\n",
       "      <td>15.12</td>\n",
       "      <td>48.92</td>\n",
       "      <td>1011.80</td>\n",
       "      <td>72.93</td>\n",
       "      <td>462.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9563</th>\n",
       "      <td>33.41</td>\n",
       "      <td>77.95</td>\n",
       "      <td>1010.30</td>\n",
       "      <td>59.72</td>\n",
       "      <td>432.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9564</th>\n",
       "      <td>15.99</td>\n",
       "      <td>43.34</td>\n",
       "      <td>1014.20</td>\n",
       "      <td>78.66</td>\n",
       "      <td>465.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9565</th>\n",
       "      <td>17.65</td>\n",
       "      <td>59.87</td>\n",
       "      <td>1018.58</td>\n",
       "      <td>94.65</td>\n",
       "      <td>450.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9566</th>\n",
       "      <td>23.68</td>\n",
       "      <td>51.30</td>\n",
       "      <td>1011.86</td>\n",
       "      <td>71.24</td>\n",
       "      <td>451.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9567 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AT      V       AP     RH      PE\n",
       "0     23.64  58.49  1011.40  74.20  445.75\n",
       "1     29.74  56.90  1007.15  41.91  438.76\n",
       "2     19.07  49.69  1007.22  76.79  453.09\n",
       "3     11.80  40.66  1017.13  97.20  464.43\n",
       "4     13.97  39.16  1016.05  84.60  470.96\n",
       "...     ...    ...      ...    ...     ...\n",
       "9562  15.12  48.92  1011.80  72.93  462.59\n",
       "9563  33.41  77.95  1010.30  59.72  432.90\n",
       "9564  15.99  43.34  1014.20  78.66  465.96\n",
       "9565  17.65  59.87  1018.58  94.65  450.93\n",
       "9566  23.68  51.30  1011.86  71.24  451.67\n",
       "\n",
       "[9567 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # pandas library use for data analysis\n",
    "import matplotlib.pyplot as plt\n",
    "#first manually convert xls to csv\n",
    "data = pd.read_csv(\"Folds_test.csv\") # import csv file as dataframe\n",
    "data # show dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=  data.head(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>V</th>\n",
       "      <th>AP</th>\n",
       "      <th>RH</th>\n",
       "      <th>PE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.64</td>\n",
       "      <td>58.49</td>\n",
       "      <td>1011.40</td>\n",
       "      <td>74.20</td>\n",
       "      <td>445.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.74</td>\n",
       "      <td>56.90</td>\n",
       "      <td>1007.15</td>\n",
       "      <td>41.91</td>\n",
       "      <td>438.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.07</td>\n",
       "      <td>49.69</td>\n",
       "      <td>1007.22</td>\n",
       "      <td>76.79</td>\n",
       "      <td>453.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.80</td>\n",
       "      <td>40.66</td>\n",
       "      <td>1017.13</td>\n",
       "      <td>97.20</td>\n",
       "      <td>464.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.97</td>\n",
       "      <td>39.16</td>\n",
       "      <td>1016.05</td>\n",
       "      <td>84.60</td>\n",
       "      <td>470.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>24.63</td>\n",
       "      <td>65.38</td>\n",
       "      <td>1010.43</td>\n",
       "      <td>40.68</td>\n",
       "      <td>446.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>19.68</td>\n",
       "      <td>62.96</td>\n",
       "      <td>1020.41</td>\n",
       "      <td>82.26</td>\n",
       "      <td>453.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>27.89</td>\n",
       "      <td>73.21</td>\n",
       "      <td>1001.32</td>\n",
       "      <td>85.88</td>\n",
       "      <td>431.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>12.82</td>\n",
       "      <td>43.50</td>\n",
       "      <td>1022.38</td>\n",
       "      <td>84.32</td>\n",
       "      <td>471.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>27.20</td>\n",
       "      <td>77.95</td>\n",
       "      <td>1009.48</td>\n",
       "      <td>78.53</td>\n",
       "      <td>433.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AT      V       AP     RH      PE\n",
       "0     23.64  58.49  1011.40  74.20  445.75\n",
       "1     29.74  56.90  1007.15  41.91  438.76\n",
       "2     19.07  49.69  1007.22  76.79  453.09\n",
       "3     11.80  40.66  1017.13  97.20  464.43\n",
       "4     13.97  39.16  1016.05  84.60  470.96\n",
       "...     ...    ...      ...    ...     ...\n",
       "1495  24.63  65.38  1010.43  40.68  446.23\n",
       "1496  19.68  62.96  1020.41  82.26  453.58\n",
       "1497  27.89  73.21  1001.32  85.88  431.59\n",
       "1498  12.82  43.50  1022.38  84.32  471.41\n",
       "1499  27.20  77.95  1009.48  78.53  433.39\n",
       "\n",
       "[1500 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # numpy is library use for matrix manipulation\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps): \n",
    "    X, y = list(), list() # initilize list for feature matrixs and labels\n",
    "    for i in range(len(sequence)): # loop through number of sequence \n",
    "        end_ix = i + n_steps # initilize end value  \n",
    "        if end_ix > len(sequence)-1: # check if end value is greater than length of sequence \n",
    "            break # use break to termninate the loop\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] # seperate features and labels\n",
    "        X.append(seq_x) # save feature in X list\n",
    "        y.append(seq_y) # save label in Y lisr\n",
    "    return np.array(X), np.array(y) # return X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1= np.array(data['AT'].values)  # save values of AT column in x1\n",
    "x2= np.array(data['V'].values)  # save values of V column in x2\n",
    "x3= np.array(data['AP'].values)  # save values of AP column in x3\n",
    "x4= np.array(data['RH'].values)  # save values of RH column in x4\n",
    "x5= np.array(data['PE'].values) # save values of PE column in x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=4\n",
    "x_1, y_1 = split_sequence(x1, seq_len) # call split sequence function \n",
    "x_2, y_2 = split_sequence(x2, seq_len) # call split sequence function \n",
    "x_3, y_3 = split_sequence(x3, seq_len) # call split sequence function \n",
    "x_4, y_4 = split_sequence(x4, seq_len) # call split sequence function \n",
    "x_5, y_5 = split_sequence(x5, seq_len) # call split sequence function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import hstack # use hstack to merger different columns\n",
    "X = hstack((x_1, x_2,x_3,x_4,x_5)) # merger 5 featuers into one matrix X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1 = y_1.reshape((-1, 1)) # convert vector row to vector column matrix\n",
    "y_2 = y_2.reshape((-1, 1)) # convert vector row to vector column matrix\n",
    "y_3 = y_3.reshape((-1, 1)) # convert vector row to vector column matrix\n",
    "y_4 = y_4.reshape((-1, 1)) # convert vector row to vector column matrix\n",
    "y_5 = y_5.reshape((-1, 1)) # convert vector row to vector column matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.column_stack((y_1,y_2,y_3,y_4,y_5)) # use column wise merge to 5 labels matrix into Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1496, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape) # \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "Y = scaler.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (1496, 20, 1) y: (1496, 5)\n",
      "(20, 1)\n",
      "5\n",
      "xtrain: (1196, 20, 1) ytrian: (1196, 5)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 20, 32)            4352      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 29,509\n",
      "Trainable params: 29,509\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1076 samples, validate on 120 samples\n",
      "Epoch 1/150\n",
      "1076/1076 - 4s - loss: 0.1744 - val_loss: 0.0556\n",
      "Epoch 2/150\n",
      "1076/1076 - 1s - loss: 0.0513 - val_loss: 0.0495\n",
      "Epoch 3/150\n",
      "1076/1076 - 1s - loss: 0.0491 - val_loss: 0.0486\n",
      "Epoch 4/150\n",
      "1076/1076 - 1s - loss: 0.0491 - val_loss: 0.0491\n",
      "Epoch 5/150\n",
      "1076/1076 - 1s - loss: 0.0491 - val_loss: 0.0481\n",
      "Epoch 6/150\n",
      "1076/1076 - 1s - loss: 0.0488 - val_loss: 0.0494\n",
      "Epoch 7/150\n",
      "1076/1076 - 1s - loss: 0.0489 - val_loss: 0.0490\n",
      "Epoch 8/150\n",
      "1076/1076 - 1s - loss: 0.0491 - val_loss: 0.0481\n",
      "Epoch 9/150\n",
      "1076/1076 - 1s - loss: 0.0491 - val_loss: 0.0483\n",
      "Epoch 10/150\n",
      "1076/1076 - 1s - loss: 0.0494 - val_loss: 0.0489\n",
      "Epoch 11/150\n",
      "1076/1076 - 1s - loss: 0.0498 - val_loss: 0.0482\n",
      "Epoch 12/150\n",
      "1076/1076 - 1s - loss: 0.0490 - val_loss: 0.0481\n",
      "Epoch 13/150\n",
      "1076/1076 - 1s - loss: 0.0487 - val_loss: 0.0482\n",
      "Epoch 14/150\n",
      "1076/1076 - 1s - loss: 0.0489 - val_loss: 0.0485\n",
      "Epoch 15/150\n",
      "1076/1076 - 1s - loss: 0.0488 - val_loss: 0.0492\n",
      "Epoch 16/150\n",
      "1076/1076 - 1s - loss: 0.0487 - val_loss: 0.0482\n",
      "Epoch 17/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0485\n",
      "Epoch 18/150\n",
      "1076/1076 - 1s - loss: 0.0488 - val_loss: 0.0486\n",
      "Epoch 19/150\n",
      "1076/1076 - 1s - loss: 0.0489 - val_loss: 0.0496\n",
      "Epoch 20/150\n",
      "1076/1076 - 1s - loss: 0.0492 - val_loss: 0.0478\n",
      "Epoch 21/150\n",
      "1076/1076 - 1s - loss: 0.0491 - val_loss: 0.0483\n",
      "Epoch 22/150\n",
      "1076/1076 - 1s - loss: 0.0489 - val_loss: 0.0479\n",
      "Epoch 23/150\n",
      "1076/1076 - 1s - loss: 0.0487 - val_loss: 0.0482\n",
      "Epoch 24/150\n",
      "1076/1076 - 1s - loss: 0.0488 - val_loss: 0.0482\n",
      "Epoch 25/150\n",
      "1076/1076 - 1s - loss: 0.0488 - val_loss: 0.0480\n",
      "Epoch 26/150\n",
      "1076/1076 - 1s - loss: 0.0488 - val_loss: 0.0483\n",
      "Epoch 27/150\n",
      "1076/1076 - 1s - loss: 0.0488 - val_loss: 0.0486\n",
      "Epoch 28/150\n",
      "1076/1076 - 1s - loss: 0.0492 - val_loss: 0.0479\n",
      "Epoch 29/150\n",
      "1076/1076 - 1s - loss: 0.0489 - val_loss: 0.0479\n",
      "Epoch 30/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0487\n",
      "Epoch 31/150\n",
      "1076/1076 - 1s - loss: 0.0487 - val_loss: 0.0486\n",
      "Epoch 32/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0493\n",
      "Epoch 33/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0481\n",
      "Epoch 34/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0494\n",
      "Epoch 35/150\n",
      "1076/1076 - 1s - loss: 0.0489 - val_loss: 0.0481\n",
      "Epoch 36/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0480\n",
      "Epoch 37/150\n",
      "1076/1076 - 1s - loss: 0.0487 - val_loss: 0.0486\n",
      "Epoch 38/150\n",
      "1076/1076 - 1s - loss: 0.0487 - val_loss: 0.0492\n",
      "Epoch 39/150\n",
      "1076/1076 - 1s - loss: 0.0488 - val_loss: 0.0490\n",
      "Epoch 40/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0481\n",
      "Epoch 41/150\n",
      "1076/1076 - 1s - loss: 0.0488 - val_loss: 0.0483\n",
      "Epoch 42/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0482\n",
      "Epoch 43/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0481\n",
      "Epoch 44/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0479\n",
      "Epoch 45/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0495\n",
      "Epoch 46/150\n",
      "1076/1076 - 1s - loss: 0.0488 - val_loss: 0.0496\n",
      "Epoch 47/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0494\n",
      "Epoch 48/150\n",
      "1076/1076 - 1s - loss: 0.0487 - val_loss: 0.0483\n",
      "Epoch 49/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0490\n",
      "Epoch 50/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0485\n",
      "Epoch 51/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0480\n",
      "Epoch 52/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0482\n",
      "Epoch 53/150\n",
      "1076/1076 - 1s - loss: 0.0488 - val_loss: 0.0480\n",
      "Epoch 54/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0508\n",
      "Epoch 55/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0480\n",
      "Epoch 56/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0483\n",
      "Epoch 57/150\n",
      "1076/1076 - 1s - loss: 0.0488 - val_loss: 0.0489\n",
      "Epoch 58/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0487\n",
      "Epoch 59/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0480\n",
      "Epoch 60/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0488\n",
      "Epoch 61/150\n",
      "1076/1076 - 1s - loss: 0.0488 - val_loss: 0.0486\n",
      "Epoch 62/150\n",
      "1076/1076 - 1s - loss: 0.0488 - val_loss: 0.0482\n",
      "Epoch 63/150\n",
      "1076/1076 - 1s - loss: 0.0487 - val_loss: 0.0483\n",
      "Epoch 64/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0483\n",
      "Epoch 65/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0480\n",
      "Epoch 66/150\n",
      "1076/1076 - 1s - loss: 0.0487 - val_loss: 0.0483\n",
      "Epoch 67/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0482\n",
      "Epoch 68/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0481\n",
      "Epoch 69/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0479\n",
      "Epoch 70/150\n",
      "1076/1076 - 1s - loss: 0.0487 - val_loss: 0.0480\n",
      "Epoch 71/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0481\n",
      "Epoch 72/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0480\n",
      "Epoch 73/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0481\n",
      "Epoch 74/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0485\n",
      "Epoch 75/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0485\n",
      "Epoch 76/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0483\n",
      "Epoch 77/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0480\n",
      "Epoch 78/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0485\n",
      "Epoch 79/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0482\n",
      "Epoch 80/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0489\n",
      "Epoch 81/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0487\n",
      "Epoch 82/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0488\n",
      "Epoch 83/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0488\n",
      "Epoch 84/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0480\n",
      "Epoch 85/150\n",
      "1076/1076 - 1s - loss: 0.0487 - val_loss: 0.0485\n",
      "Epoch 86/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0487\n",
      "Epoch 87/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0482\n",
      "Epoch 88/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0492\n",
      "Epoch 89/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0485\n",
      "Epoch 90/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0496\n",
      "Epoch 91/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0486\n",
      "Epoch 92/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0496\n",
      "Epoch 93/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0479\n",
      "Epoch 94/150\n",
      "1076/1076 - 1s - loss: 0.0487 - val_loss: 0.0489\n",
      "Epoch 95/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0487\n",
      "Epoch 96/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0483\n",
      "Epoch 97/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0487\n",
      "Epoch 98/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0479\n",
      "Epoch 99/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0493\n",
      "Epoch 100/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0479\n",
      "Epoch 101/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0491\n",
      "Epoch 102/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0482\n",
      "Epoch 103/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0487\n",
      "Epoch 104/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0489\n",
      "Epoch 105/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0484\n",
      "Epoch 106/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0482\n",
      "Epoch 107/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0486\n",
      "Epoch 108/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0481\n",
      "Epoch 109/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0482\n",
      "Epoch 110/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0480\n",
      "Epoch 111/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0488\n",
      "Epoch 112/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0490\n",
      "Epoch 113/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0493\n",
      "Epoch 114/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0479\n",
      "Epoch 115/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0493\n",
      "Epoch 116/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0485\n",
      "Epoch 117/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0480\n",
      "Epoch 118/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0492\n",
      "Epoch 120/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0490\n",
      "Epoch 121/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0480\n",
      "Epoch 122/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0480\n",
      "Epoch 123/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0488\n",
      "Epoch 124/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0493\n",
      "Epoch 125/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0487\n",
      "Epoch 126/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0487\n",
      "Epoch 127/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0482\n",
      "Epoch 128/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0487\n",
      "Epoch 129/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0488\n",
      "Epoch 130/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0489\n",
      "Epoch 131/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0486\n",
      "Epoch 132/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0483\n",
      "Epoch 133/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0484\n",
      "Epoch 134/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0495\n",
      "Epoch 135/150\n",
      "1076/1076 - 1s - loss: 0.0486 - val_loss: 0.0489\n",
      "Epoch 136/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0485\n",
      "Epoch 137/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0482\n",
      "Epoch 138/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0481\n",
      "Epoch 139/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0483\n",
      "Epoch 140/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0481\n",
      "Epoch 141/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0484\n",
      "Epoch 142/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0484\n",
      "Epoch 143/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0483\n",
      "Epoch 144/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0481\n",
      "Epoch 145/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0487\n",
      "Epoch 146/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0486\n",
      "Epoch 147/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0488\n",
      "Epoch 148/150\n",
      "1076/1076 - 1s - loss: 0.0485 - val_loss: 0.0480\n",
      "Epoch 149/150\n",
      "1076/1076 - 1s - loss: 0.0484 - val_loss: 0.0487\n",
      "Epoch 150/150\n",
      "1076/1076 - 1s - loss: 0.0483 - val_loss: 0.0481\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential # import keras sequential library\n",
    "from tensorflow.keras.layers import Dense, LSTM # import keras dense and lstm layers\n",
    "from sklearn.model_selection import train_test_split # import train and test library\n",
    "from sklearn.metrics import mean_squared_error # import metrics\n",
    "import matplotlib.pyplot as plt # for visualization purpose\n",
    "\n",
    "x = X.reshape(X.shape[0], X.shape[1], 1) # As lstm need three dimensions, so add one extra dimension\n",
    "print(\"x:\", x.shape, \"y:\", Y.shape) # print shape\n",
    " \n",
    "in_dim = (x.shape[1], x.shape[2]) # save in input dimension\n",
    "out_dim = Y.shape[1] # save in out_dim variable\n",
    "print(in_dim) # print \n",
    "print(out_dim) # print \n",
    "\n",
    "xtrain, xtest, ytrain, ytest=train_test_split(x, Y, test_size=0.2) # call train_test_split function\n",
    "print(\"xtrain:\", xtrain.shape, \"ytrian:\", ytrain.shape) # print \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential() # call sequential constructot\n",
    "model.add(LSTM(32, input_shape=in_dim,return_sequences=True, activation=\"relu\")) # configure lstm layers with 32 hidden neurons\n",
    "model.add(LSTM(64,activation=\"relu\")) # 2nd layer is 64 hidden neurons\n",
    "model.add(Dense(out_dim)) # fully connected layer with 5 variable\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\") # use optimizer adam and loss is mse\n",
    "model.summary() # overall summary \n",
    "\n",
    "history=model.fit(xtrain, ytrain, validation_split=0.1,epochs=150, batch_size=32, verbose=2,shuffle=True) # train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  18.778934,   53.601868, 1012.5408  ,   73.417885,  454.28574 ],\n",
       "       [  19.024998,   53.754227, 1012.34216 ,   72.87467 ,  453.8499  ],\n",
       "       [  19.038015,   53.59161 , 1012.1509  ,   72.4501  ,  453.6806  ],\n",
       "       ...,\n",
       "       [  18.891443,   53.630363, 1012.3956  ,   73.04527 ,  454.01852 ],\n",
       "       [  19.364384,   54.01406 , 1012.0667  ,   72.163864,  453.23416 ],\n",
       "       [  19.063692,   53.702618, 1012.22797 ,   72.644714,  453.74335 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testpred = model.predict(xtest) # validate model on test data \n",
    "testpred = scaler.inverse_transform(testpred) # denormalized the data\n",
    "testpred # show the test prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67989997],\n",
       "       [0.37668021],\n",
       "       [0.73991872],\n",
       "       [0.38168178],\n",
       "       [0.69044144],\n",
       "       [0.52790952],\n",
       "       [0.85789858],\n",
       "       [0.60616563],\n",
       "       [0.4708589 ],\n",
       "       [0.60940695],\n",
       "       [0.17868098],\n",
       "       [0.50792434],\n",
       "       [0.71950509],\n",
       "       [0.61062591],\n",
       "       [0.8294032 ],\n",
       "       [0.85007278],\n",
       "       [0.30240844],\n",
       "       [0.53370386],\n",
       "       [0.13324783],\n",
       "       [0.41955252]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest[0] # for one row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_row = np.expand_dims(xtest[0], axis=0) #  add dummy dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  18.778934,   53.60187 , 1012.5408  ,   73.417885,  454.28574 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testpred_row = model.predict(test_one_row) # test with one row \n",
    "testpred_row = scaler.inverse_transform(testpred_row) # denormalized the prediction\n",
    "testpred_row # show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
